#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç»¼åˆåŠŸèƒ½æ¼”ç¤ºè„šæœ¬
å±•ç¤ºæ‰€æœ‰æ–°å¢åŠŸèƒ½çš„ä½¿ç”¨æ–¹æ³•
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from chinese_herbal_sentiment import (
    SentimentAnalysis,
    KeywordExtraction,
    SupplyChainRegression,
    PredictionService,
    TimeSeriesAnalyzer
)


def demo_regression_analysis():
    """æ¼”ç¤ºå›å½’åˆ†æåŠŸèƒ½"""
    print("=" * 60)
    print("1. å›å½’åˆ†ææ¼”ç¤º")
    print("=" * 60)

    # åˆ›å»ºå›å½’åˆ†æå™¨
    regressor = SupplyChainRegression(model_type='linear')

    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    print("ç”Ÿæˆä¾›åº”é“¾è´¨é‡è¯„ä»·æ¨¡æ‹Ÿæ•°æ®...")
    data = regressor.generate_supply_chain_data(n_samples=500)
    print(f"æ•°æ®ç»´åº¦: {data.shape}")

    # å‡†å¤‡æ•°æ®
    feature_columns = [col for col in data.columns
                      if col not in ['service_quality', 'enterprise_size', 'product_type', 'region']]
    categorical_columns = ['enterprise_size', 'product_type', 'region']

    X, y = regressor.prepare_data(
        data=data,
        target_column='service_quality',
        feature_columns=feature_columns + categorical_columns,
        categorical_columns=categorical_columns
    )

    # è®­ç»ƒæ¨¡å‹
    print("è®­ç»ƒå›å½’æ¨¡å‹...")
    results = regressor.train(X, y, test_size=0.2, random_state=42)

    print(f"è®­ç»ƒé›†RÂ²: {results['train_r2']:.4f}")
    print(f"æµ‹è¯•é›†RÂ²: {results['test_r2']:.4f}")
    print(f"æµ‹è¯•é›†MSE: {results['test_mse']:.4f}")

    # ç‰¹å¾é‡è¦æ€§
    importance_df = regressor.feature_importance()
    print("\nå‰5ä¸ªé‡è¦ç‰¹å¾:")
    for _, row in importance_df.head(5).iterrows():
        print(f"  - {row['feature']}: {row['coefficient']:.4f}")

    # é¢„æµ‹æ¼”ç¤º
    print("\né¢„æµ‹æ¼”ç¤º:")
    X_new = X[:3]
    predictions = regressor.predict(X_new)
    actual_values = y[:3]

    for i, (pred, actual) in enumerate(zip(predictions, actual_values)):
        print(f"  æ ·æœ¬{i+1}: é¢„æµ‹={pred:.3f}, å®é™…={actual:.3f}")

    # ä¿å­˜æ¨¡å‹å’Œç»“æœ
    os.makedirs('output', exist_ok=True)
    regressor.save_model('output/demo_regression_model.pkl')
    regressor.visualize_results('output/regression_analysis.png')
    regressor.generate_report('output/regression_report.md')

    print("å›å½’åˆ†æå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°outputç›®å½•")
    return regressor


def demo_prediction_service():
    """æ¼”ç¤ºé¢„æµ‹æœåŠ¡åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("2. é¢„æµ‹æœåŠ¡æ¼”ç¤º")
    print("=" * 60)

    # åˆ›å»ºé¢„æµ‹æœåŠ¡
    service = PredictionService()

    # æ¼”ç¤ºæ–‡æœ¬
    demo_texts = [
        "è¿™ä¸ªä¸­è¯è´¨é‡å¾ˆå¥½ï¼Œæ•ˆæœä¸é”™ï¼Œç‰©æµä¹Ÿå¾ˆå¿«",
        "åŒ…è£…ç ´æŸä¸¥é‡ï¼Œäº§å“è´¨é‡å¾ˆå·®ï¼Œä¸æ¨èè´­ä¹°",
        "æœåŠ¡æ€åº¦ä¸€èˆ¬ï¼Œäº§å“è¿˜å¯ä»¥ï¼Œä»·æ ¼åˆç†",
        "éå¸¸æ»¡æ„çš„è´­ä¹°ä½“éªŒï¼Œäº§å“è´¨é‡ä¼˜ç§€ï¼Œä¼šå†æ¬¡è´­ä¹°"
    ]

    print("æ¼”ç¤ºæ–‡æœ¬:")
    for i, text in enumerate(demo_texts, 1):
        print(f"  {i}. {text}")

    # æƒ…æ„Ÿåˆ†æ
    print("\næƒ…æ„Ÿåˆ†æç»“æœ:")
    sentiment_results = service.predict_sentiment(demo_texts, methods=['dictionary'])

    for i, (text, pred, score) in enumerate(zip(
        demo_texts,
        sentiment_results['predictions']['dictionary']['labels'],
        sentiment_results['predictions']['dictionary']['scores']
    ), 1):
        sentiment_name = "æ­£é¢" if pred == 1 else ("è´Ÿé¢" if pred == -1 else "ä¸­æ€§")
        print(f"  {i}. {sentiment_name} (è¯„åˆ†: {score:.3f})")

    # å…³é”®è¯æå–
    print("\nå…³é”®è¯æå–ç»“æœ:")
    keyword_results = service.extract_keywords_batch(demo_texts, methods=['tfidf'])

    if 'tfidf' in keyword_results['keywords']:
        for i, keywords in enumerate(keyword_results['keywords']['tfidf'][:3], 1):
            top_keywords = [word for word, score in keywords[:5]]
            print(f"  æ–‡æœ¬{i}: {', '.join(top_keywords)}")

    # ç»¼åˆåˆ†æ
    print("\nç»¼åˆåˆ†æ:")
    comprehensive_results = service.analyze_comprehensive(
        texts=demo_texts[:2],
        include_sentiment=True,
        include_keywords=True
    )

    print(f"  åˆ†æäº† {comprehensive_results['texts_count']} æ¡æ–‡æœ¬")
    print(f"  åŒ…å«çš„åˆ†æç±»å‹: {list(comprehensive_results['results'].keys())}")

    # æ¨¡å‹ä¿¡æ¯
    model_info = service.get_model_info()
    print(f"\nå¯ç”¨çš„åˆ†æå™¨: {model_info['available_analyzers']}")

    print("é¢„æµ‹æœåŠ¡æ¼”ç¤ºå®Œæˆ!")
    return service


def demo_time_series_analysis():
    """æ¼”ç¤ºæ—¶é—´åºåˆ—åˆ†æåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("3. æ—¶é—´åºåˆ—åˆ†ææ¼”ç¤º")
    print("=" * 60)

    # åˆ›å»ºæ—¶é—´åºåˆ—åˆ†æå™¨
    analyzer = TimeSeriesAnalyzer()

    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    print("ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®...")
    sample_data = analyzer.generate_sample_data(
        start_date='2023-01-01',
        periods=180,  # åŠå¹´æ•°æ®
        frequency='D'
    )

    print(f"æ•°æ®ç»´åº¦: {sample_data.shape}")
    print(f"æ—¶é—´èŒƒå›´: {sample_data['date'].min()} åˆ° {sample_data['date'].max()}")

    # åŠ è½½æ•°æ®
    analyzer.load_data(sample_data, 'date', 'sentiment_score')

    # è¶‹åŠ¿åˆ†æ
    print("\nè¶‹åŠ¿åˆ†æ...")
    trend_results = analyzer.trend_analysis(method='linear')
    print(f"  è¶‹åŠ¿æ–¹å‘: {trend_results['trend_direction']}")
    print(f"  è¶‹åŠ¿å¼ºåº¦: {trend_results['trend_strength']:.4f}")
    if 'slope' in trend_results:
        print(f"  å˜åŒ–ç‡: {trend_results['slope']:.6f} å•ä½/å¤©")

    # å­£èŠ‚æ€§åˆ†æ
    print("\nå­£èŠ‚æ€§åˆ†æ...")
    seasonal_results = analyzer.seasonal_analysis()
    if 'seasonal_strength' in seasonal_results:
        print(f"  å­£èŠ‚æ€§å¼ºåº¦: {seasonal_results['seasonal_strength']:.4f}")
    print(f"  æ£€æµ‹å‘¨æœŸ: {seasonal_results.get('period', 'N/A')}")

    # é¢„æµ‹åˆ†æ
    print("\né¢„æµ‹åˆ†æ...")
    forecast_results = analyzer.forecast(periods=14, method='auto')
    analyzer.forecast_results = forecast_results

    print(f"  é¢„æµ‹æ–¹æ³•: {forecast_results['method']}")
    print(f"  é¢„æµ‹æœŸæ•°: {forecast_results['periods']}")
    predictions = forecast_results['predictions']
    print(f"  é¢„æµ‹å‡å€¼: {np.mean(predictions):.4f}")
    print(f"  é¢„æµ‹èŒƒå›´: {min(predictions):.4f} - {max(predictions):.4f}")

    # å¼‚å¸¸å€¼æ£€æµ‹
    print("\nå¼‚å¸¸å€¼æ£€æµ‹...")
    anomalies = analyzer.detect_anomalies(method='iqr')
    print(f"  æ£€æµ‹åˆ° {anomalies['anomaly_count']} ä¸ªå¼‚å¸¸å€¼")
    print(f"  å¼‚å¸¸å€¼æ¯”ä¾‹: {anomalies['anomaly_percentage']:.2f}%")

    if anomalies['anomaly_count'] > 0:
        print("  ä¸»è¦å¼‚å¸¸å€¼:")
        for i, (date, value) in enumerate(zip(
            anomalies['anomaly_dates'][:3],
            anomalies['anomaly_values'][:3]
        )):
            print(f"    - {date}: {value:.4f}")

    # å¯è§†åŒ–å’ŒæŠ¥å‘Š
    try:
        analyzer.visualize_analysis(
            include_trend=True,
            include_seasonal=True,
            include_forecast=True,
            include_anomalies=True,
            save_path='output/time_series_analysis.png'
        )

        analyzer.generate_report('output/time_series_report.md')
        print("\næ—¶é—´åºåˆ—åˆ†æå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°outputç›®å½•")
    except Exception as e:
        print(f"\nå¯è§†åŒ–æˆ–æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

    return analyzer


def demo_api_usage():
    """æ¼”ç¤ºAPIä½¿ç”¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    print("\n" + "=" * 60)
    print("4. APIæœåŠ¡æ¼”ç¤º")
    print("=" * 60)

    try:
        from chinese_herbal_sentiment.api import run_server
        print("APIæ¨¡å—å¯ç”¨!")
        print("è¦å¯åŠ¨APIæœåŠ¡å™¨ï¼Œè¯·è¿è¡Œ:")
        print("  python -m chinese_herbal_sentiment.api.app")
        print("æˆ–è€…:")
        print("  from chinese_herbal_sentiment.api import run_server")
        print("  run_server()")
        print("\nAPIå°†åœ¨ http://localhost:8000 æä¾›æœåŠ¡")
        print("APIæ–‡æ¡£: http://localhost:8000/docs")

        # å±•ç¤ºAPIç«¯ç‚¹
        print("\nå¯ç”¨çš„APIç«¯ç‚¹:")
        endpoints = [
            "GET  /health - å¥åº·æ£€æŸ¥",
            "POST /api/v1/sentiment/analyze - æƒ…æ„Ÿåˆ†æ",
            "POST /api/v1/keywords/extract - å…³é”®è¯æå–",
            "POST /api/v1/quality/predict - è´¨é‡é¢„æµ‹",
            "POST /api/v1/timeseries/analyze - æ—¶é—´åºåˆ—åˆ†æ",
            "POST /api/v1/analyze/comprehensive - ç»¼åˆåˆ†æ",
            "GET  /api/v1/models/info - æ¨¡å‹ä¿¡æ¯",
            "GET  /api/v1/predictions/history - é¢„æµ‹å†å²"
        ]

        for endpoint in endpoints:
            print(f"  {endpoint}")

    except ImportError:
        print("APIæ¨¡å—ä¸å¯ç”¨ (éœ€è¦å®‰è£… fastapi å’Œ uvicorn)")
        print("å®‰è£…å‘½ä»¤: pip install fastapi uvicorn")


def demo_integration_example():
    """æ¼”ç¤ºé›†æˆä½¿ç”¨ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("5. é›†æˆä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)

    print("åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„åˆ†ææµç¨‹...")

    # 1. å‡†å¤‡æ•°æ®
    print("\n1. å‡†å¤‡è¯„è®ºæ•°æ®...")
    reviews_data = {
        'date': pd.date_range('2023-01-01', periods=30, freq='D'),
        'review': [
            f"ç¬¬{i}å¤©çš„ä¸­è¯æè¯„è®ºï¼Œè´¨é‡{'å¾ˆå¥½' if i % 3 == 0 else 'ä¸€èˆ¬' if i % 3 == 1 else 'è¾ƒå·®'}"
            for i in range(30)
        ],
        'rating': np.random.randint(3, 6, 30)  # 3-5åˆ†è¯„åˆ†
    }
    df = pd.DataFrame(reviews_data)

    # 2. æƒ…æ„Ÿåˆ†æ
    print("\n2. æ‰¹é‡æƒ…æ„Ÿåˆ†æ...")
    sentiment_analyzer = SentimentAnalysis()
    sentiment_scores = []

    for review in df['review']:
        score = sentiment_analyzer.dictionary_based_analysis(review)
        sentiment_scores.append(score)

    df['sentiment_score'] = sentiment_scores
    print(f"  å¹³å‡æƒ…æ„Ÿå¾—åˆ†: {np.mean(sentiment_scores):.3f}")

    # 3. å…³é”®è¯æå–
    print("\n3. æå–å…³é”®è¯...")
    keyword_extractor = KeywordExtraction()
    all_keywords = keyword_extractor.tfidf_extraction(df['review'].tolist(), top_k=10)

    print("  ä¸»è¦å…³é”®è¯:")
    if all_keywords and len(all_keywords) > 0:
        for item in all_keywords[:5]:
            if isinstance(item, tuple) and len(item) == 2:
                word, score = item
                print(f"    - {word}: {score:.3f}")
            else:
                print(f"    - {item}")

    # 4. æ—¶é—´åºåˆ—åˆ†æ
    print("\n4. æ—¶é—´åºåˆ—è¶‹åŠ¿åˆ†æ...")
    ts_analyzer = TimeSeriesAnalyzer()

    # ä½¿ç”¨è¯„åˆ†ä½œä¸ºæ—¶é—´åºåˆ—æ•°æ®
    ts_data = df[['date', 'rating']].copy()
    ts_analyzer.load_data(ts_data, 'date', 'rating')

    trend_result = ts_analyzer.trend_analysis(method='linear')
    print(f"  è¯„åˆ†è¶‹åŠ¿: {trend_result['trend_direction']}")
    print(f"  è¶‹åŠ¿å¼ºåº¦: {trend_result['trend_strength']:.3f}")

    # 5. è´¨é‡é¢„æµ‹
    print("\n5. åŸºäºè¯„è®ºé¢„æµ‹ä¾›åº”é“¾è´¨é‡...")

    # åˆ›å»ºç‰¹å¾ï¼ˆåŸºäºæƒ…æ„Ÿåˆ†æç»“æœï¼‰
    avg_sentiment = np.mean(sentiment_scores)
    avg_rating = np.mean(df['rating'])

    # æ¨¡æ‹Ÿä¾›åº”é“¾ç‰¹å¾
    features = {
        'material_quality': 5 + avg_sentiment * 2,  # åŸºäºæƒ…æ„Ÿè°ƒæ•´
        'technology': 7.0 + avg_rating - 4,  # åŸºäºè¯„åˆ†è°ƒæ•´
        'delivery_speed': 7.5,
        'after_sales_service': 6.5 + avg_sentiment,
        'material_consistency': 7.2,
        'production_efficiency': 7.4,
        'quality_standard': 7.6,
        'product_consistency': 7.3,
        'processing_environment': 7.1,
        'packaging': 7.8,
        'order_accuracy': 8.0,
        'inventory_management': 7.4,
        'information_transparency': 6.9
    }

    # ä½¿ç”¨é¢„æµ‹æœåŠ¡
    prediction_service = PredictionService()

    # å°è¯•åˆ›å»ºå’Œè®­ç»ƒå›å½’æ¨¡å‹ç”¨äºé¢„æµ‹
    try:
        regressor = SupplyChainRegression()
        sample_data = regressor.generate_supply_chain_data(200)

        feature_cols = [col for col in sample_data.columns
                       if col not in ['service_quality', 'enterprise_size', 'product_type', 'region']]

        X, y = regressor.prepare_data(sample_data, 'service_quality', feature_cols)
        regressor.train(X, y)

        # é¢„æµ‹è´¨é‡è¯„åˆ†
        feature_vector = np.array([[features[col] if col in features else 7.5
                                  for col in feature_cols]])
        predicted_quality = regressor.predict(feature_vector)[0]

        print(f"  é¢„æµ‹ä¾›åº”é“¾è´¨é‡è¯„åˆ†: {predicted_quality:.2f}/10")

    except Exception as e:
        print(f"  è´¨é‡é¢„æµ‹å¤±è´¥: {e}")

    # 6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("\n6. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")

    report = f"""
    # ä¸­è¯æè¯„è®ºç»¼åˆåˆ†ææŠ¥å‘Š

    ## æ•°æ®æ¦‚å†µ
    - åˆ†ææœŸé—´: {df['date'].min().strftime('%Y-%m-%d')} è‡³ {df['date'].max().strftime('%Y-%m-%d')}
    - è¯„è®ºæ•°é‡: {len(df)} æ¡
    - å¹³å‡è¯„åˆ†: {avg_rating:.2f}/5
    - å¹³å‡æƒ…æ„Ÿå¾—åˆ†: {avg_sentiment:.3f}

    ## å…³é”®å‘ç°
    - è¯„åˆ†è¶‹åŠ¿: {trend_result['trend_direction']}
    - è¶‹åŠ¿å¼ºåº¦: {trend_result['trend_strength']:.3f}

        ## ä¸»è¦å…³é”®è¯
    """
    
    if all_keywords and len(all_keywords) > 0:
        for item in all_keywords[:5]:
            if isinstance(item, tuple) and len(item) == 2:
                word, score = item
                report += f"    - {word}: {score:.3f}\n"
            else:
                report += f"    - {item}\n"

    # ä¿å­˜æŠ¥å‘Š
    os.makedirs('output', exist_ok=True)
    with open('output/comprehensive_analysis_report.md', 'w', encoding='utf-8') as f:
        f.write(report)

    print("ç»¼åˆåˆ†æå®Œæˆ! æŠ¥å‘Šå·²ä¿å­˜åˆ° output/comprehensive_analysis_report.md")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸŒ¿ ä¸­è¯ææƒ…æ„Ÿåˆ†æç³»ç»Ÿ - ç»¼åˆåŠŸèƒ½æ¼”ç¤º")
    print("=" * 80)

    try:
        # 1. å›å½’åˆ†ææ¼”ç¤º
        regressor = demo_regression_analysis()

        # 2. é¢„æµ‹æœåŠ¡æ¼”ç¤º
        service = demo_prediction_service()

        # 3. æ—¶é—´åºåˆ—åˆ†ææ¼”ç¤º
        analyzer = demo_time_series_analysis()

        # 4. APIä½¿ç”¨æ¼”ç¤º
        demo_api_usage()

        # 5. é›†æˆä½¿ç”¨ç¤ºä¾‹
        demo_integration_example()

        print("\n" + "=" * 80)
        print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
        print("=" * 80)
        print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        output_files = [
            "output/demo_regression_model.pkl - å›å½’æ¨¡å‹",
            "output/regression_analysis.png - å›å½’åˆ†æå›¾è¡¨",
            "output/regression_report.md - å›å½’åˆ†ææŠ¥å‘Š",
            "output/time_series_analysis.png - æ—¶é—´åºåˆ—å›¾è¡¨",
            "output/time_series_report.md - æ—¶é—´åºåˆ—æŠ¥å‘Š",
            "output/comprehensive_analysis_report.md - ç»¼åˆåˆ†ææŠ¥å‘Š"
        ]

        for file_desc in output_files:
            print(f"  â€¢ {file_desc}")

        print("\nğŸš€ è¦å¯åŠ¨APIæœåŠ¡å™¨ï¼Œè¯·è¿è¡Œ:")
        print("  python examples/comprehensive_demo.py --api")
        print("æˆ–è€…:")
        print("  python -c \"from chinese_herbal_sentiment.api import run_server; run_server()\"")

    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--api":
        # å¯åŠ¨APIæœåŠ¡å™¨
        try:
            from chinese_herbal_sentiment.api import run_server
            print("ğŸš€ å¯åŠ¨APIæœåŠ¡å™¨...")
            run_server(host="127.0.0.1", port=8000, reload=True)
        except ImportError:
            print("âŒ APIåŠŸèƒ½ä¸å¯ç”¨ã€‚è¯·å®‰è£…: pip install fastapi uvicorn")
    else:
        # è¿è¡Œæ¼”ç¤º
        main()
